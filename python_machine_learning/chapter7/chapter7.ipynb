{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Learning\n",
    "\n",
    "The intuition is to combine different classifiers into a meta-classifier that has a better performance than any individual classifier alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9.2\n",
      "0.17\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "print np.__version__\n",
    "print sklearn.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Majority Vote Concept\n",
    "\n",
    "The concept of weighted majority vote.   \n",
    "3 Classifier classifies this observation as class [ 0, 0, 1 ] respectively. If the weight of these 3 classifiers are [ 0.2, 0.2, 0.6 ] then this observation will be classified as class 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assign a list of weight, whose shape is equivalent to x\n",
    "np.argmax( np.bincount( x = [ 0, 0, 1 ], weights = [ 0.2, 0.2, 0.6 ] ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When class becomes probability ( probability of being classified as class 0 and class1 )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.58  0.42]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "ex = np.array( [ [ 0.9, 0.1 ], [ 0.8, 0.2 ], [ 0.4, 0.6 ] ] )\n",
    "p = np.average( ex, axis = 0, weights = [ 0.2, 0.2, 0.6 ] )\n",
    "print(p)\n",
    "print( np.argmax(p) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Preparing the Dataset and Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load iris data. Use only two of the features and work on classifying two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X, y = iris.data[ 50:, [1, 2] ], iris.target[50:]\n",
    "le = LabelEncoder()\n",
    "y  = le.fit_transform(y)\n",
    "\n",
    "# split the data into 50% training / 50% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size = 0.5, random_state = 1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train three different classifiers : 1) logistic regression ; 2) decision tree ; 3) k nearest neighbors. Look at their individual performance via a 10 fold cross-validation before combining them into an emsemble classifer.\n",
    "\n",
    "Note that unlike tree-base algorithms, logistic regression and k nearest neighbors are not scale-invariant, thus it's a good habit to work with standardized features ( use a Pipeline )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC: 0.92 (+/- 0.20) [Logistic Regression]\n",
      "ROC AUC: 0.92 (+/- 0.15) [Decision Tree]\n",
      "ROC AUC: 0.93 (+/- 0.10) [KNN]\n"
     ]
    }
   ],
   "source": [
    "clf1 = LogisticRegression( penalty = 'l2', \n",
    "                           C = 0.001, \n",
    "                           random_state = 0 )\n",
    "\n",
    "clf2 = DecisionTreeClassifier(max_depth = 1, \n",
    "                              criterion = 'entropy', \n",
    "                              random_state = 0 )\n",
    "\n",
    "clf3 = KNeighborsClassifier( n_neighbors = 1, \n",
    "                             p = 2, \n",
    "                             metric = 'minkowski')\n",
    "\n",
    "pipe1 = Pipeline( [ [ 'sc', StandardScaler() ],\n",
    "                    [ 'clf', clf1 ] ] )\n",
    "pipe3 = Pipeline( [ [ 'sc', StandardScaler() ],\n",
    "                    [ 'clf', clf3 ] ] )\n",
    "\n",
    "clf_labels = [ 'Logistic Regression', 'Decision Tree', 'KNN' ]\n",
    "\n",
    "for clf, label in zip( [ pipe1, clf2, pipe3 ], clf_labels ) :\n",
    "    # scores returns an array of float\n",
    "    scores = cross_val_score( estimator = clf, X = X_train, y = y_train, \n",
    "                              cv = 10, scoring = 'roc_auc' )\n",
    "    print( \"ROC AUC: %0.2f (+/- %0.2f) [%s]\" % \n",
    "           ( scores.mean(), scores.std(), label ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Majority Voter\n",
    "\n",
    "Implement majority voter from scratch. A quick tour on some of the part that may be confusing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import _name_estimators # generate names for estimators "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- sklearn's `_name_estimators` example usage.\n",
    "\n",
    "`_name_estimators` returns a list of tuples, we can convert it into a dictionary, where the key is the name of the estimator provided by the `_name_estimator` function and the value is its corresponding estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decisiontreeclassifier': DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=1,\n",
       "             max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             presort=False, random_state=0, splitter='best'),\n",
       " 'pipeline-1': Pipeline(steps=[['sc', StandardScaler(copy=True, with_mean=True, with_std=True)], ['clf', LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "           penalty='l2', random_state=0, solver='liblinear', tol=0.0001,\n",
       "           verbose=0, warm_start=False)]]),\n",
       " 'pipeline-2': Pipeline(steps=[['sc', StandardScaler(copy=True, with_mean=True, with_std=True)], ['clf', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "            metric_params=None, n_jobs=1, n_neighbors=1, p=2,\n",
       "            weights='uniform')]])}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifiers = [ pipe1, clf2, pipe3 ]\n",
    "{ key: value for key, value in _name_estimators( classifiers ) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- python's exception. Raise a value error.\n",
    "\n",
    "Side note on the difference between str and repr in python. https://www.youtube.com/watch?v=5cvM-crlDvg.\n",
    "str is meant to be readable to the users, while repre is meant to be unambiguous ( easier to debug, since it's kind of like showing you the functional call that creates the variable ).   \n",
    "`%r` represents the object as repr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "vote must be 'probability' or 'classlabel'; got (1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-cb1b8c2e6961>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mvote\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'probability'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'classlabel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     raise ValueError( \"vote must be 'probability' or 'classlabel'\"\n\u001b[0;32m----> 4\u001b[0;31m                       \"; got (%r)\" % vote )\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: vote must be 'probability' or 'classlabel'; got (1)"
     ]
    }
   ],
   "source": [
    "vote = 1\n",
    "if vote not in ('probability', 'classlabel') :\n",
    "    raise ValueError( \"vote must be 'probability' or 'classlabel'\"\n",
    "                      \"; got (%r)\" % vote )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.base import clone\n",
    "lablenc_ = LabelEncoder()\n",
    "lablenc_.fit(y)\n",
    "classes_ = lablenc_.classes_ # obtains the class for the labelEncoder\n",
    "classifiers_ = []\n",
    "for clf in classifiers:\n",
    "    fitted_clf = clone(clf).fit( X, lablenc_.transform(y))\n",
    "    classifiers_.append(fitted_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [0, 0, 0]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array( [clf.predict(X) for clf in classifiers_ ] ).T[ :2, : ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = np.asarray([clf.predict(X)\n",
    "                                      for clf in self.classifiers_]).T\n",
    "\"\"\"\n",
    "            maj_vote = np.apply_along_axis(\n",
    "                                      lambda x:\n",
    "                                      np.argmax(np.bincount(x,\n",
    "                                                weights=self.weights)),\n",
    "                                      axis=1,\n",
    "                                      arr=predictions)\n",
    "        maj_vote = self.lablenc_.inverse_transform(maj_vote)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#mv_clf = MajorityVoteClassifier(\n",
    "#                classifiers=[pipe1, clf2, pipe3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
