{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Learning\n",
    "\n",
    "The intuition is to combine different classifiers into a meta-classifier that has a better performance than any individual classifier alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9.2\n",
      "0.17\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "print np.__version__\n",
    "print sklearn.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Majority Vote Concept\n",
    "\n",
    "The concept of weighted majority vote.   \n",
    "3 Classifier classifies this observation as class [ 0, 0, 1 ] respectively. If the weight of these 3 classifiers are [ 0.2, 0.2, 0.6 ] then this observation will be classified as class 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assign a list of weight, whose shape is equivalent to x\n",
    "np.argmax( np.bincount( [ 0, 0, 1 ], weights = [ 0.2, 0.2, 0.6 ] ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When class becomes probability ( probability of being classified as class 0 and class1 )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.58  0.42]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "ex = np.array( [ [ 0.9, 0.1 ], [ 0.8, 0.2 ], [ 0.4, 0.6 ] ] )\n",
    "p = np.average( ex, axis = 0, weights = [ 0.2, 0.2, 0.6 ] )\n",
    "print(p)\n",
    "print( np.argmax(p) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Preparing the Dataset and Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load iris data. Use only two of the features and work on classifying two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X, y = iris.data[ 50:, [1, 2] ], iris.target[50:]\n",
    "le = LabelEncoder()\n",
    "y  = le.fit_transform(y)\n",
    "\n",
    "# split the data into 50% training / 50% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size = 0.5, random_state = 1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train three different classifiers : 1) logistic regression ; 2) decision tree ; 3) k nearest neighbors. Look at their individual performance via a 10 fold cross-validation before combining them into an emsemble classifer.\n",
    "\n",
    "Note that unlike tree-base algorithms, logistic regression and k nearest neighbors are not scale-invariant, thus it's a good habit to work with standardized features ( use a Pipeline )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC: 0.92 (+/- 0.20) [Logistic Regression]\n",
      "ROC AUC: 0.92 (+/- 0.15) [Decision Tree]\n",
      "ROC AUC: 0.93 (+/- 0.10) [KNN]\n"
     ]
    }
   ],
   "source": [
    "clf1 = LogisticRegression( penalty = 'l2', \n",
    "                           C = 0.001, \n",
    "                           random_state = 0 )\n",
    "\n",
    "clf2 = DecisionTreeClassifier(max_depth = 1, \n",
    "                              criterion = 'entropy', \n",
    "                              random_state = 0 )\n",
    "\n",
    "clf3 = KNeighborsClassifier( n_neighbors = 1, \n",
    "                             p = 2, \n",
    "                             metric = 'minkowski')\n",
    "\n",
    "pipe1 = Pipeline( [ [ 'sc', StandardScaler() ],\n",
    "                    [ 'clf', clf1 ] ] )\n",
    "pipe3 = Pipeline( [ [ 'sc', StandardScaler() ],\n",
    "                    [ 'clf', clf3 ] ] )\n",
    "\n",
    "clf_labels = [ 'Logistic Regression', 'Decision Tree', 'KNN' ]\n",
    "\n",
    "for clf, label in zip( [ pipe1, clf2, pipe3 ], clf_labels ) :\n",
    "    # scores returns an array of float\n",
    "    scores = cross_val_score( estimator = clf, X = X_train, y = y_train, \n",
    "                              cv = 10, scoring = 'roc_auc' )\n",
    "    print( \"ROC AUC: %0.2f (+/- %0.2f) [%s]\" % \n",
    "           ( scores.mean(), scores.std(), label ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Majority Voter\n",
    "\n",
    "Implement majority voter from scratch. A quick tour on some of the part that may be confusing.\n",
    "\n",
    "### sklearn's `_name_estimators` example usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`_name_estimators` returns a list of tuples, we can convert it into a dictionary, where the key is the name of the estimator provided by the `_name_estimator` function and the value is its corresponding estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decisiontreeclassifier': DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=1,\n",
       "             max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             presort=False, random_state=0, splitter='best'),\n",
       " 'pipeline-1': Pipeline(steps=[['sc', StandardScaler(copy=True, with_mean=True, with_std=True)], ['clf', LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "           penalty='l2', random_state=0, solver='liblinear', tol=0.0001,\n",
       "           verbose=0, warm_start=False)]]),\n",
       " 'pipeline-2': Pipeline(steps=[['sc', StandardScaler(copy=True, with_mean=True, with_std=True)], ['clf', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "            metric_params=None, n_jobs=1, n_neighbors=1, p=2,\n",
       "            weights='uniform')]])}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import _name_estimators # generate names for estimators \n",
    "classifiers = [ pipe1, clf2, pipe3 ]\n",
    "named_classifiers = { key: value for key, value in _name_estimators( classifiers ) }\n",
    "named_classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### python's exception. Raise a value error.\n",
    "\n",
    "Side note on the difference between str and repr in python. https://www.youtube.com/watch?v=5cvM-crlDvg.\n",
    "str is meant to be readable to the users, while repre is meant to be unambiguous ( easier to debug, since it's kind of like showing you the functional call that creates the variable ).   \n",
    "`%r` represents the object as repr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "vote must be 'probability' or 'classlabel'; got (1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-e9cd48f6005e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mvote\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'probability'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'classlabel'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     raise ValueError( \"vote must be 'probability' or 'classlabel'\"\n\u001b[0;32m----> 4\u001b[0;31m                       \"; got (%r)\" % vote )\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: vote must be 'probability' or 'classlabel'; got (1)"
     ]
    }
   ],
   "source": [
    "vote = 1\n",
    "if vote not in ('probability', 'classlabel') :\n",
    "    raise ValueError( \"vote must be 'probability' or 'classlabel'\"\n",
    "                      \"; got (%r)\" % vote )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop through all the classfiers and fit all the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.base import clone\n",
    "lablenc_ = LabelEncoder()\n",
    "lablenc_.fit(y)\n",
    "classes_ = lablenc_.classes_ # obtains the class for the labelEncoder\n",
    "classifiers_ = []\n",
    "for clf in classifiers:\n",
    "    fitted_clf = clone(clf).fit( X, lablenc_.transform(y) )\n",
    "    classifiers_.append(fitted_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code when you only want to obtain the final ensembled-class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = np.asarray([ clf.predict(X) for clf in classifiers_ ]).T\n",
    "# give a function to apply along the specified axis of the array\n",
    "maj_vote = np.apply_along_axis( lambda x:\n",
    "                                np.argmax( np.bincount( x, weights = weights ) ),\n",
    "                                axis = 1,\n",
    "                                arr = predictions )\n",
    "maj_vote = lablenc_.inverse_transform(maj_vote)\n",
    "maj_vote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'steps': [['sc', StandardScaler(copy=True, with_mean=True, with_std=True)],\n",
       "  ['clf',\n",
       "   LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "             penalty='l2', random_state=0, solver='liblinear', tol=0.0001,\n",
       "             verbose=0, warm_start=False)]]}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifiers_[0].get_params( deep = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [1, 1, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print out the first five predicted rows for all three classifiers\n",
    "np.asarray( [clf.predict(X) for clf in classifiers_ ] ).T[ :5, : ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.49879125,  0.50120875],\n",
       "        [ 0.5011151 ,  0.4988849 ],\n",
       "        [ 0.49756567,  0.50243433],\n",
       "        [ 0.51680288,  0.48319712],\n",
       "        [ 0.50434615,  0.49565385],\n",
       "        [ 0.50550797,  0.49449203],\n",
       "        [ 0.49769299,  0.50230701],\n",
       "        [ 0.5238264 ,  0.4761736 ],\n",
       "        [ 0.50324794,  0.49675206],\n",
       "        [ 0.51357472,  0.48642528],\n",
       "        [ 0.52589051,  0.47410949],\n",
       "        [ 0.50679702,  0.49320298],\n",
       "        [ 0.51789983,  0.48210017],\n",
       "        [ 0.50208605,  0.49791395],\n",
       "        [ 0.51486292,  0.48513708],\n",
       "        [ 0.50337526,  0.49662474],\n",
       "        [ 0.5033116 ,  0.4966884 ],\n",
       "        [ 0.51125229,  0.48874771],\n",
       "        [ 0.51209547,  0.48790453],\n",
       "        [ 0.51576937,  0.48423063],\n",
       "        [ 0.49762933,  0.50237067],\n",
       "        [ 0.51131592,  0.48868408],\n",
       "        [ 0.50415519,  0.49584481],\n",
       "        [ 0.50318429,  0.49681571],\n",
       "        [ 0.50673338,  0.49326662],\n",
       "        [ 0.50447346,  0.49552654],\n",
       "        [ 0.50202239,  0.49797761],\n",
       "        [ 0.49750201,  0.50249799],\n",
       "        [ 0.50440981,  0.49559019],\n",
       "        [ 0.51931443,  0.48068557],\n",
       "        [ 0.51802698,  0.48197302],\n",
       "        [ 0.51918731,  0.48081269],\n",
       "        [ 0.51357472,  0.48642528],\n",
       "        [ 0.49963488,  0.50036512],\n",
       "        [ 0.5033116 ,  0.4966884 ],\n",
       "        [ 0.49891856,  0.50108144],\n",
       "        [ 0.49988952,  0.50011048],\n",
       "        [ 0.51215909,  0.48784091],\n",
       "        [ 0.5079587 ,  0.4920413 ],\n",
       "        [ 0.51460851,  0.48539149],\n",
       "        [ 0.50886574,  0.49113426],\n",
       "        [ 0.50214971,  0.49785029],\n",
       "        [ 0.5135111 ,  0.4864889 ],\n",
       "        [ 0.52492206,  0.47507794],\n",
       "        [ 0.51009089,  0.48990911],\n",
       "        [ 0.50679702,  0.49320298],\n",
       "        [ 0.50789506,  0.49210494],\n",
       "        [ 0.50673338,  0.49326662],\n",
       "        [ 0.52620794,  0.47379206],\n",
       "        [ 0.51015452,  0.48984548],\n",
       "        [ 0.4825949 ,  0.5174051 ],\n",
       "        [ 0.49963488,  0.50036512],\n",
       "        [ 0.48704751,  0.51295249],\n",
       "        [ 0.49162946,  0.50837054],\n",
       "        [ 0.48820873,  0.51179127],\n",
       "        [ 0.47892359,  0.52107641],\n",
       "        [ 0.5088021 ,  0.4911979 ],\n",
       "        [ 0.48350115,  0.51649885],\n",
       "        [ 0.49369824,  0.50630176],\n",
       "        [ 0.47814505,  0.52185495],\n",
       "        [ 0.49414379,  0.50585621],\n",
       "        [ 0.49731104,  0.50268896],\n",
       "        [ 0.4916931 ,  0.5083069 ],\n",
       "        [ 0.50299332,  0.49700668],\n",
       "        [ 0.49853661,  0.50146339],\n",
       "        [ 0.49182039,  0.50817961],\n",
       "        [ 0.4916931 ,  0.5083069 ],\n",
       "        [ 0.46900279,  0.53099721],\n",
       "        [ 0.47982934,  0.52017066],\n",
       "        [ 0.50628784,  0.49371216],\n",
       "        [ 0.48717475,  0.51282525],\n",
       "        [ 0.50086047,  0.49913953],\n",
       "        [ 0.47995645,  0.52004355],\n",
       "        [ 0.50195873,  0.49804127],\n",
       "        [ 0.48607726,  0.51392274],\n",
       "        [ 0.48369192,  0.51630808],\n",
       "        [ 0.50202239,  0.49797761],\n",
       "        [ 0.49866393,  0.50133607],\n",
       "        [ 0.49272746,  0.50727254],\n",
       "        [ 0.48820873,  0.51179127],\n",
       "        [ 0.48692028,  0.51307972],\n",
       "        [ 0.47247663,  0.52752337],\n",
       "        [ 0.49272746,  0.50727254],\n",
       "        [ 0.49853661,  0.50146339],\n",
       "        [ 0.49492367,  0.50507633],\n",
       "        [ 0.4847255 ,  0.5152745 ],\n",
       "        [ 0.48614087,  0.51385913],\n",
       "        [ 0.49059518,  0.50940482],\n",
       "        [ 0.49982586,  0.50017414],\n",
       "        [ 0.49175674,  0.50824326],\n",
       "        [ 0.48943371,  0.51056629],\n",
       "        [ 0.49524193,  0.50475807],\n",
       "        [ 0.49963488,  0.50036512],\n",
       "        [ 0.48485271,  0.51514729],\n",
       "        [ 0.48607726,  0.51392274],\n",
       "        [ 0.49517828,  0.50482172],\n",
       "        [ 0.50299332,  0.49700668],\n",
       "        [ 0.49517828,  0.50482172],\n",
       "        [ 0.48846323,  0.51153677],\n",
       "        [ 0.49634013,  0.50365987]],\n",
       "\n",
       "       [[ 0.97777778,  0.02222222],\n",
       "        [ 0.97777778,  0.02222222],\n",
       "        [ 0.10909091,  0.89090909],\n",
       "        [ 0.97777778,  0.02222222],\n",
       "        [ 0.97777778,  0.02222222],\n",
       "        [ 0.97777778,  0.02222222],\n",
       "        [ 0.97777778,  0.02222222],\n",
       "        [ 0.97777778,  0.02222222],\n",
       "        [ 0.97777778,  0.02222222],\n",
       "        [ 0.97777778,  0.02222222],\n",
       "        [ 0.97777778,  0.02222222],\n",
       "        [ 0.97777778,  0.02222222],\n",
       "        [ 0.97777778,  0.02222222],\n",
       "        [ 0.97777778,  0.02222222],\n",
       "        [ 0.97777778,  0.02222222],\n",
       "        [ 0.97777778,  0.02222222],\n",
       "        [ 0.97777778,  0.02222222],\n",
       "        [ 0.97777778,  0.02222222],\n",
       "        [ 0.97777778,  0.02222222],\n",
       "        [ 0.97777778,  0.02222222],\n",
       "        [ 0.10909091,  0.89090909],\n",
       "        [ 0.97777778,  0.02222222],\n",
       "        [ 0.10909091,  0.89090909],\n",
       "        [ 0.97777778,  0.02222222],\n",
       "        [ 0.97777778,  0.02222222],\n",
       "        [ 0.97777778,  0.02222222],\n",
       "        [ 0.10909091,  0.89090909],\n",
       "        [ 0.10909091,  0.89090909],\n",
       "        [ 0.97777778,  0.02222222],\n",
       "        [ 0.97777778,  0.02222222],\n",
       "        [ 0.97777778,  0.02222222],\n",
       "        [ 0.97777778,  0.02222222],\n",
       "        [ 0.97777778,  0.02222222],\n",
       "        [ 0.10909091,  0.89090909],\n",
       "        [ 0.97777778,  0.02222222],\n",
       "        [ 0.97777778,  0.02222222],\n",
       "        [ 0.97777778,  0.02222222],\n",
       "        [ 0.97777778,  0.02222222],\n",
       "        [ 0.97777778,  0.02222222],\n",
       "        [ 0.97777778,  0.02222222],\n",
       "        [ 0.97777778,  0.02222222],\n",
       "        [ 0.97777778,  0.02222222],\n",
       "        [ 0.97777778,  0.02222222],\n",
       "        [ 0.97777778,  0.02222222],\n",
       "        [ 0.97777778,  0.02222222],\n",
       "        [ 0.97777778,  0.02222222],\n",
       "        [ 0.97777778,  0.02222222],\n",
       "        [ 0.97777778,  0.02222222],\n",
       "        [ 0.97777778,  0.02222222],\n",
       "        [ 0.97777778,  0.02222222],\n",
       "        [ 0.10909091,  0.89090909],\n",
       "        [ 0.10909091,  0.89090909],\n",
       "        [ 0.10909091,  0.89090909],\n",
       "        [ 0.10909091,  0.89090909],\n",
       "        [ 0.10909091,  0.89090909],\n",
       "        [ 0.10909091,  0.89090909],\n",
       "        [ 0.97777778,  0.02222222],\n",
       "        [ 0.10909091,  0.89090909],\n",
       "        [ 0.10909091,  0.89090909],\n",
       "        [ 0.10909091,  0.89090909],\n",
       "        [ 0.10909091,  0.89090909],\n",
       "        [ 0.10909091,  0.89090909],\n",
       "        [ 0.10909091,  0.89090909],\n",
       "        [ 0.10909091,  0.89090909],\n",
       "        [ 0.10909091,  0.89090909],\n",
       "        [ 0.10909091,  0.89090909],\n",
       "        [ 0.10909091,  0.89090909],\n",
       "        [ 0.10909091,  0.89090909],\n",
       "        [ 0.10909091,  0.89090909],\n",
       "        [ 0.10909091,  0.89090909],\n",
       "        [ 0.10909091,  0.89090909],\n",
       "        [ 0.10909091,  0.89090909],\n",
       "        [ 0.10909091,  0.89090909],\n",
       "        [ 0.10909091,  0.89090909],\n",
       "        [ 0.10909091,  0.89090909],\n",
       "        [ 0.10909091,  0.89090909],\n",
       "        [ 0.10909091,  0.89090909],\n",
       "        [ 0.10909091,  0.89090909],\n",
       "        [ 0.10909091,  0.89090909],\n",
       "        [ 0.10909091,  0.89090909],\n",
       "        [ 0.10909091,  0.89090909],\n",
       "        [ 0.10909091,  0.89090909],\n",
       "        [ 0.10909091,  0.89090909],\n",
       "        [ 0.10909091,  0.89090909],\n",
       "        [ 0.10909091,  0.89090909],\n",
       "        [ 0.10909091,  0.89090909],\n",
       "        [ 0.10909091,  0.89090909],\n",
       "        [ 0.10909091,  0.89090909],\n",
       "        [ 0.10909091,  0.89090909],\n",
       "        [ 0.10909091,  0.89090909],\n",
       "        [ 0.10909091,  0.89090909],\n",
       "        [ 0.10909091,  0.89090909],\n",
       "        [ 0.10909091,  0.89090909],\n",
       "        [ 0.10909091,  0.89090909],\n",
       "        [ 0.10909091,  0.89090909],\n",
       "        [ 0.10909091,  0.89090909],\n",
       "        [ 0.10909091,  0.89090909],\n",
       "        [ 0.10909091,  0.89090909],\n",
       "        [ 0.10909091,  0.89090909],\n",
       "        [ 0.10909091,  0.89090909]],\n",
       "\n",
       "       [[ 1.        ,  0.        ],\n",
       "        [ 1.        ,  0.        ],\n",
       "        [ 1.        ,  0.        ],\n",
       "        [ 1.        ,  0.        ],\n",
       "        [ 1.        ,  0.        ],\n",
       "        [ 1.        ,  0.        ],\n",
       "        [ 1.        ,  0.        ],\n",
       "        [ 1.        ,  0.        ],\n",
       "        [ 1.        ,  0.        ],\n",
       "        [ 1.        ,  0.        ],\n",
       "        [ 1.        ,  0.        ],\n",
       "        [ 1.        ,  0.        ],\n",
       "        [ 1.        ,  0.        ],\n",
       "        [ 1.        ,  0.        ],\n",
       "        [ 1.        ,  0.        ],\n",
       "        [ 1.        ,  0.        ],\n",
       "        [ 1.        ,  0.        ],\n",
       "        [ 1.        ,  0.        ],\n",
       "        [ 1.        ,  0.        ],\n",
       "        [ 1.        ,  0.        ],\n",
       "        [ 1.        ,  0.        ],\n",
       "        [ 1.        ,  0.        ],\n",
       "        [ 1.        ,  0.        ],\n",
       "        [ 1.        ,  0.        ],\n",
       "        [ 1.        ,  0.        ],\n",
       "        [ 1.        ,  0.        ],\n",
       "        [ 1.        ,  0.        ],\n",
       "        [ 1.        ,  0.        ],\n",
       "        [ 1.        ,  0.        ],\n",
       "        [ 1.        ,  0.        ],\n",
       "        [ 1.        ,  0.        ],\n",
       "        [ 1.        ,  0.        ],\n",
       "        [ 1.        ,  0.        ],\n",
       "        [ 1.        ,  0.        ],\n",
       "        [ 1.        ,  0.        ],\n",
       "        [ 1.        ,  0.        ],\n",
       "        [ 1.        ,  0.        ],\n",
       "        [ 1.        ,  0.        ],\n",
       "        [ 1.        ,  0.        ],\n",
       "        [ 1.        ,  0.        ],\n",
       "        [ 1.        ,  0.        ],\n",
       "        [ 1.        ,  0.        ],\n",
       "        [ 1.        ,  0.        ],\n",
       "        [ 1.        ,  0.        ],\n",
       "        [ 1.        ,  0.        ],\n",
       "        [ 1.        ,  0.        ],\n",
       "        [ 1.        ,  0.        ],\n",
       "        [ 1.        ,  0.        ],\n",
       "        [ 1.        ,  0.        ],\n",
       "        [ 1.        ,  0.        ],\n",
       "        [ 0.        ,  1.        ],\n",
       "        [ 1.        ,  0.        ],\n",
       "        [ 0.        ,  1.        ],\n",
       "        [ 0.        ,  1.        ],\n",
       "        [ 0.        ,  1.        ],\n",
       "        [ 0.        ,  1.        ],\n",
       "        [ 0.        ,  1.        ],\n",
       "        [ 0.        ,  1.        ],\n",
       "        [ 0.        ,  1.        ],\n",
       "        [ 0.        ,  1.        ],\n",
       "        [ 0.        ,  1.        ],\n",
       "        [ 0.        ,  1.        ],\n",
       "        [ 0.        ,  1.        ],\n",
       "        [ 0.        ,  1.        ],\n",
       "        [ 0.        ,  1.        ],\n",
       "        [ 0.        ,  1.        ],\n",
       "        [ 0.        ,  1.        ],\n",
       "        [ 0.        ,  1.        ],\n",
       "        [ 0.        ,  1.        ],\n",
       "        [ 0.        ,  1.        ],\n",
       "        [ 0.        ,  1.        ],\n",
       "        [ 0.        ,  1.        ],\n",
       "        [ 0.        ,  1.        ],\n",
       "        [ 0.        ,  1.        ],\n",
       "        [ 0.        ,  1.        ],\n",
       "        [ 0.        ,  1.        ],\n",
       "        [ 1.        ,  0.        ],\n",
       "        [ 0.        ,  1.        ],\n",
       "        [ 0.        ,  1.        ],\n",
       "        [ 0.        ,  1.        ],\n",
       "        [ 0.        ,  1.        ],\n",
       "        [ 0.        ,  1.        ],\n",
       "        [ 0.        ,  1.        ],\n",
       "        [ 0.        ,  1.        ],\n",
       "        [ 0.        ,  1.        ],\n",
       "        [ 0.        ,  1.        ],\n",
       "        [ 0.        ,  1.        ],\n",
       "        [ 0.        ,  1.        ],\n",
       "        [ 0.        ,  1.        ],\n",
       "        [ 0.        ,  1.        ],\n",
       "        [ 0.        ,  1.        ],\n",
       "        [ 0.        ,  1.        ],\n",
       "        [ 1.        ,  0.        ],\n",
       "        [ 0.        ,  1.        ],\n",
       "        [ 0.        ,  1.        ],\n",
       "        [ 0.        ,  1.        ],\n",
       "        [ 0.        ,  1.        ],\n",
       "        [ 0.        ,  1.        ],\n",
       "        [ 0.        ,  1.        ],\n",
       "        [ 0.        ,  1.        ]]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probas = np.asarray( [ clf.predict_proba(X) for clf in classifiers_ ] )\n",
    "probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 100, 2)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probas.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.5000001,  0.4999999],\n",
       "       [ 0.5      ,  0.5      ],\n",
       "       [ 0.53     ,  0.47     ]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_proba = np.average( probas, axis = 1, weights = None )\n",
    "avg_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = [ 0.4, 0.4, 0.3 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#mv_clf = MajorityVoteClassifier(\n",
    "#                classifiers=[pipe1, clf2, pipe3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline-2 Pipeline(steps=[['sc', StandardScaler(copy=True, with_mean=True, with_std=True)], ['clf', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=1, p=2,\n",
      "           weights='uniform')]])\n",
      "pipeline-1 Pipeline(steps=[['sc', StandardScaler(copy=True, with_mean=True, with_std=True)], ['clf', LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=0, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)]])\n",
      "decisiontreeclassifier DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=1,\n",
      "            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=0, splitter='best')\n"
     ]
    }
   ],
   "source": [
    "for name, step in named_classifiers.iteritems():\n",
    "    print name, step"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
