{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras\n",
    "\n",
    "http://keras.io/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers.core import Dense\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing the toy dataset. Note that the iris data is sorted by the label. In such cases, the `validation_split` that we'll later use in `model.fit` will not work correctly. Thus, we should shuffle data before training.\n",
    "\n",
    "https://github.com/fchollet/keras/issues/68"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "shuffle = np.arange(len(iris.data))\n",
    "np.random.shuffle(shuffle)\n",
    "X = iris.data[shuffle]\n",
    "y = iris.target[shuffle]\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size = 0.3, random_state = 0 )\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train_sd = sc.fit_transform(X_train)\n",
    "X_test_sd  = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to convert the class labels to one-hot format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train_ohe = np_utils.to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The core datastructure of Keras is a model. e.g. `Sequential()`; a linear pile of layers.\n",
    "- Stacking layers is as easy as `.add()`.\n",
    "    - Since the first layer that we add is the input layer, we have to make sure that the input_dim attribute matches the number of features (columns) in the training set. Also, the number of output units (output_dim) and input units (input_dim) of two consecutive layers has to match. Finally, the number of units in the output layer should be equal to the number of unique class labels.\n",
    "- Once your model looks good, configure its learning process with `.compile()`.\n",
    "    - We can define our own optimizer to train the model. Here we use `SGD()`; stochastic gradient descent optimization. For the SGD, we can then define the `lr`; learning rate. The `decay`; weight decay. The `momentum`; This simply adds a fraction m of the previous weight update to the current one.\n",
    "    - We set the cost or loss function. Here `categorical_crossentropy`; Simply refers to the cost function of logistic regression, note that the prediction is generalized to multi-class via the softmax activation in the final layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# this is equivalent to adding two hidden layers with 50 hidden units each\n",
    "model.add( Dense( input_dim = X_train_sd.shape[1], output_dim = 50,\n",
    "                  init = 'uniform', activation = 'sigmoid' ) )\n",
    "model.add( Dense( input_dim = 50, output_dim = 50,\n",
    "                  init = 'uniform', activation = 'sigmoid' ) )\n",
    "model.add( Dense( input_dim = 50, output_dim = y_train_ohe.shape[1],\n",
    "                  init = 'uniform', activation = 'softmax' ) )\n",
    "sgd = SGD( lr = 0.001, decay = 1e-7, momentum = .9 )\n",
    "model.compile( loss = 'categorical_crossentropy', optimizer = sgd )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After compiling the model, we can now train the model by calling `.fit()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A simpler version for the small iris data\n",
    "model = Sequential()                                                       \n",
    "model.add( Dense( input_dim = X_train_sd.shape[1], output_dim = y_train_ohe.shape[1], \n",
    "                  init = 'uniform', activation = 'softmax' ) )                                          \n",
    "model.compile( loss = 'mean_squared_error', optimizer = 'sgd' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x110d9b550>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    X_train_sd, \n",
    "    y_train_ohe,\n",
    "    nb_epoch = 30,\n",
    "    batch_size = 1, # minibatch training \n",
    "    verbose = 0, # 1 for printing out the cost function\n",
    "    validation_split = 0.1 # reserve 10 percent of the data for validation after each epoch\n",
    "    # show_accuracy = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the class labels and print out the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 0, 2, 0, 1, 2, 2, 2, 2, 0, 2, 0, 1, 0, 0, 2, 1, 2, 1, 0, 1, 2,\n",
       "       0, 2, 2, 2, 2, 1, 1, 0, 1, 2, 2, 2, 0, 2, 0, 0, 2, 1, 2, 0, 0, 2, 2,\n",
       "       2, 0, 1, 0, 0, 2, 0, 1, 2, 1, 1, 2, 1, 2, 0, 1, 0, 0, 1, 2, 0, 0, 0,\n",
       "       0, 2, 2, 0, 0, 1, 0, 1, 2, 0, 2, 0, 2, 1, 0, 0, 0, 2, 0, 1, 2, 1, 2,\n",
       "       1, 2, 2, 2, 0, 2, 1, 1, 0, 2, 2, 2, 1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.evaluate( X_test, y_test, batch_size = 30 )\n",
    "y_train_pred = model.predict_classes( X_train_sd, verbose = 0 )\n",
    "y_train_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 89.52%\n"
     ]
    }
   ],
   "source": [
    "train_acc = float( np.sum( y_train == y_train_pred ) ) / X_train.shape[0]\n",
    "print( 'Training accuracy: %.2f%%' % ( train_acc * 100 ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Future work, Kaggle Example.\n",
    "\n",
    "https://www.kaggle.com/c/otto-group-product-classification-challenge/forums/t/13632/achieve-0-48-in-5-min-with-a-deep-net-feat-batchnorm-prelu"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
