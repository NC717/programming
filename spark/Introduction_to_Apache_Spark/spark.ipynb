{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation \n",
    "\n",
    "Download pyspark from anaconda.\n",
    "\n",
    "```shell\n",
    "conda install -y -c anaconda-cluster spark\n",
    "```\n",
    "\n",
    "Download the latest version of apache spark from this [link](https://spark.apache.org/downloads.html). For the package type choose the package `pre-built for Hadoop 2.6` and leave the rest untouched. After downloading it, unzip it and place it in your home directory.\n",
    "\n",
    "Then you need to define these environment variables before starting the notebook.\n",
    "\n",
    "```shell\n",
    "export SPARK_HOME=~/spark\n",
    "export PYSPARK_PYTHON=python3\n",
    "export PYTHONPATH=$SPARK_HOME/python/:$PYTHONPATH\n",
    "export PACKAGES=\"com.databricks:spark-csv_2.11:1.4.0\"\n",
    "export PYSPARK_SUBMIT_ARGS=\"--packages ${PACKAGES} pyspark-shell\"\n",
    "```\n",
    "\n",
    "In Unix/Mac, this can be done in .bashrc or .bash_profile.\n",
    "\n",
    "- [link](http://people.duke.edu/~ccc14/sta-663-2016/21A_Introduction_To_Spark.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the environment that we're going to be using for this course is the Python programming interface to Spark called, pySpark. It provides an easy to use programming abstraction and parallel runtime. Again, you just say, here's an operation, run it on all of the data. The key concept that we're going to use is spark's notion of data frames.\n",
    "\n",
    "When you run a Spark program, it actually consists of two programs. There's a driver program and there's a workers program. The driver program runs on one machine. And the worker program runs either on cluster nodes or in local threads on the same machine.\n",
    "\n",
    "The data frames that you create are distributed across all of the workers.\n",
    "\n",
    "A Spark program first creates a `SparkContext` object. And the `SparkContext` tells Spark how and where\n",
    "to access a cluster. Now if you use the pySpark shell or you use the Databricks Community Edition environment, they both automatically create the SparkContext for you.\n",
    "\n",
    "If you're using iPython or you're writing a Spark program, separated from the shell or the Databricks Community Edition environment, then you have to create a new `SparkContext` yourself.\n",
    "\n",
    "Next the program creates a SQL context object. You use the SQL context object to create data frames."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
