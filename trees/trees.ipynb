{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "@import url('http://fonts.googleapis.com/css?family=Source+Code+Pro');\n",
       "@import url('http://fonts.googleapis.com/css?family=Vollkorn');\n",
       "@import url('http://fonts.googleapis.com/css?family=Arimo');\n",
       "@import url('http://fonts.googleapis.com/css?family=Fira_sans');\n",
       "\n",
       "    div.cell{\n",
       "        width: 1000px;\n",
       "        margin-left: 0% !important;\n",
       "        margin-right: auto;\n",
       "    }\n",
       "    div.text_cell code {\n",
       "        background: transparent;\n",
       "        color: #000000;\n",
       "        font-weight: 600;\n",
       "        font-size: 12pt;\n",
       "        font-style: bold;\n",
       "        font-family:  'Source Code Pro', Consolas, monocco, monospace;\n",
       "   }\n",
       "    h1 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "\t}\n",
       "\t\n",
       "    div.input_area {\n",
       "        background: #F6F6F9;\n",
       "        border: 1px solid #586e75;\n",
       "    }\n",
       "\n",
       "    .text_cell_render h1 {\n",
       "        font-weight: 200;\n",
       "        font-size: 30pt;\n",
       "        line-height: 100%;\n",
       "        color:#c76c0c;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 1em;\n",
       "        display: block;\n",
       "        white-space: wrap;\n",
       "        text-align: left;\n",
       "    } \n",
       "    h2 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "        text-align: left;\n",
       "    }\n",
       "    .text_cell_render h2 {\n",
       "        font-weight: 200;\n",
       "        font-size: 16pt;\n",
       "        font-style: italic;\n",
       "        line-height: 100%;\n",
       "        color:#c76c0c;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 1.5em;\n",
       "        display: block;\n",
       "        white-space: wrap;\n",
       "        text-align: left;\n",
       "    } \n",
       "    h3 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "    }\n",
       "    .text_cell_render h3 {\n",
       "        font-weight: 200;\n",
       "        font-size: 14pt;\n",
       "        line-height: 100%;\n",
       "        color:#d77c0c;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 2em;\n",
       "        display: block;\n",
       "        white-space: wrap;\n",
       "        text-align: left;\n",
       "    }\n",
       "    h4 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "    }\n",
       "    .text_cell_render h4 {\n",
       "        font-weight: 100;\n",
       "        font-size: 14pt;\n",
       "        color:#d77c0c;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "        white-space: nowrap;\n",
       "    }\n",
       "    h5 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "    }\n",
       "    .text_cell_render h5 {\n",
       "        font-weight: 200;\n",
       "        font-style: normal;\n",
       "        color: #1d3b84;\n",
       "        font-size: 16pt;\n",
       "        margin-bottom: 0em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "        white-space: nowrap;\n",
       "    }\n",
       "    div.text_cell_render{\n",
       "        font-family: 'Fira sans', verdana,arial,sans-serif;\n",
       "        line-height: 125%;\n",
       "        font-size: 115%;\n",
       "        text-align:justify;\n",
       "        text-justify:inter-word;\n",
       "    }\n",
       "    div.output_subarea.output_text.output_pyout {\n",
       "        overflow-x: auto;\n",
       "        overflow-y: scroll;\n",
       "        max-height: 50000px;\n",
       "    }\n",
       "    div.output_subarea.output_stream.output_stdout.output_text {\n",
       "        overflow-x: auto;\n",
       "        overflow-y: scroll;\n",
       "        max-height: 50000px;\n",
       "    }\n",
       "    div.output_wrapper{\n",
       "        margin-top:0.2em;\n",
       "        margin-bottom:0.2em;\n",
       "}\n",
       "\n",
       "    code{\n",
       "      font-size: 70%;\n",
       "    }\n",
       "    .rendered_html code{\n",
       "    background-color: transparent;\n",
       "    }\n",
       "    ul{\n",
       "        margin: 2em;\n",
       "    }\n",
       "    ul li{\n",
       "        padding-left: 0.5em; \n",
       "        margin-bottom: 0.5em; \n",
       "        margin-top: 0.5em; \n",
       "    }\n",
       "    ul li li{\n",
       "        padding-left: 0.2em; \n",
       "        margin-bottom: 0.2em; \n",
       "        margin-top: 0.2em; \n",
       "    }\n",
       "    ol{\n",
       "        margin: 2em;\n",
       "    }\n",
       "    ol li{\n",
       "        padding-left: 0.5em; \n",
       "        margin-bottom: 0.5em; \n",
       "        margin-top: 0.5em; \n",
       "    }\n",
       "    ul li{\n",
       "        padding-left: 0.5em; \n",
       "        margin-bottom: 0.5em; \n",
       "        margin-top: 0.2em; \n",
       "    }\n",
       "    a:link{\n",
       "       font-weight: bold;\n",
       "       color:#447adb;\n",
       "    }\n",
       "    a:visited{\n",
       "       font-weight: bold;\n",
       "       color: #1d3b84;\n",
       "    }\n",
       "    a:hover{\n",
       "       font-weight: bold;\n",
       "       color: #1d3b84;\n",
       "    }\n",
       "    a:focus{\n",
       "       font-weight: bold;\n",
       "       color:#447adb;\n",
       "    }\n",
       "    a:active{\n",
       "       font-weight: bold;\n",
       "       color:#447adb;\n",
       "    }\n",
       "    .rendered_html :link {\n",
       "       text-decoration: underline; \n",
       "    }\n",
       "    .rendered_html :hover {\n",
       "       text-decoration: none; \n",
       "    }\n",
       "    .rendered_html :visited {\n",
       "      text-decoration: none;\n",
       "    }\n",
       "    .rendered_html :focus {\n",
       "      text-decoration: none;\n",
       "    }\n",
       "    .rendered_html :active {\n",
       "      text-decoration: none;\n",
       "    }\n",
       "    .warning{\n",
       "        color: rgb( 240, 20, 20 )\n",
       "    } \n",
       "    hr {\n",
       "      color: #f3f3f3;\n",
       "      background-color: #f3f3f3;\n",
       "      height: 1px;\n",
       "    }\n",
       "    blockquote{\n",
       "      display:block;\n",
       "      background: #fcfcfc;\n",
       "      border-left: 5px solid #c76c0c;\n",
       "      font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "      width:680px;\n",
       "      padding: 10px 10px 10px 10px;\n",
       "      text-align:justify;\n",
       "      text-justify:inter-word;\n",
       "      }\n",
       "      blockquote p {\n",
       "        margin-bottom: 0;\n",
       "        line-height: 125%;\n",
       "        font-size: 100%;\n",
       "      }\n",
       "</style>\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\"]\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    scale:100,\n",
       "                        availableFonts: [],\n",
       "                        preferredFont:null,\n",
       "                        webFont: \"TeX\",\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code for loading the format for the notebook\n",
    "import os\n",
    "\n",
    "# path : store the current path to convert back to it later\n",
    "path = os.getcwd()\n",
    "os.chdir('../notebook_format')\n",
    "from formats import load_style\n",
    "load_style()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.chdir(path)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# some jupyter notebook magic to automatically reload the modules\n",
    "# before executing the code\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Decision Tree\n",
    "\n",
    "Decision trees are extremely useful, very popular because of its interpretability and used a lot in the finance industry, particularly when looking at evaluating loan applications. So lets say, I want to buy a house and needs to take a loan from the bank. So the bank is going to look at my history record, like my credit, what has it been like in the past? How much money do I make? And other personal information about me, like my gender, age and so on. And the bank is then going to take that information and try to make a prediction as to whether loaning me money is a risky thing or not.\n",
    "\n",
    "Below is an example decision tree of how the bank might arrive at its decision.\n",
    "\n",
    "<img src='tree.png' width = 700 height = 700  >\n",
    "\n",
    "So, the algorithm works by starting at the top of the tree (the root node) and it will go down one branch or one path of this decision tree and ask a series of questions, e.g. If somebody has a credit that's fair and he is applying for a short-term loan (less than three year in this case), then this is considered a risky application (the leaf node). \n",
    "\n",
    "We can then look at another person whose credit is poor, his income is high but he is still applying for a short term loan, and the decision tree will still consider it as a risky application. \n",
    "\n",
    "In general we're going to be given an input $x_i$. In this case it has values to each one of those input features that say credit poor, income high, and term five years longer term loan. And then we traverse down the path of the tree to make a prediction whether the loan would be safe. Our our task our course is to learn the tree that let's us make predictions about our data.\n",
    "\n",
    "A core for learning a decision tree is a very simple recursive algorithm.\n",
    "\n",
    "1. Start at the tree's root node (the one at the very top). It is called the parent node if the node is not at the top of the tree.\n",
    "2. Select the best feature that splits the data for the current node. Here we'll be using binary classification tree, which simply splits the parent node into a constant of two child nodes.\n",
    "3. For each split of the tree, we either make our predictions when the tree can't be further splitted or we return to step 1 and 2 to recursively split the tree deeper and deeper. \n",
    "\n",
    "There are a few points that we need to make more concrete. We have to decide how to pick the feature to split on. And then since this is a recursive algorithm here, we have to figure out when to stop the recursion, or simply when to not go and split another node in the tree.\n",
    "\n",
    "If that explanation is still kind of blurry, try out this cool interactive visualization of how decision tree works. [A Visual Introduction to Machine Learning](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting criteria for classification trees\n",
    "\n",
    "The first question is what is the best feature to split on and how do we measure that?\n",
    "\n",
    "Well, the simplest measurement is the **classification error rate: ** Fraction of observations in a region that don't belong to the most common class.\n",
    "\n",
    "This simply means we'll predict all the samples with the majority class, and compute the classification error rate by: input number of the non-majority class divided by the total number of inputs at the current node.\n",
    "\n",
    "Here's a concrete example, pretend that we are predicting whether someone buys an iPhone or an Android:\n",
    "\n",
    "- At a particular node, there are **25 observations** (phone buyers), of whom **10 bought iPhones and 15 bought Androids**.\n",
    "- Since the majority class is **Android**, that's our prediction for all 25 observations, thus the classification error rate is **10/25 = 40%**.\n",
    "\n",
    "Our goal in making splits is to **reduce the classification error rate**. Let's try splitting on gender and suppose that it gave us the result of:\n",
    "\n",
    "- **Males:** 2 iPhones and 12 Androids, thus the predicted class is Android\n",
    "- **Females:** 8 iPhones and 3 Androids, thus the predicted class is iPhone\n",
    "- Classification error rate after this split would be **5/25 = 20%**\n",
    "\n",
    "Compare that with a split on age:\n",
    "\n",
    "- **30 or younger:** 4 iPhones and 8 Androids, thus the predicted class is Android\n",
    "- **31 or older:** 6 iPhones and 7 Androids, thus the predicted class is Android\n",
    "- Classification error rate after this split would be **10/25 = 40%**\n",
    "\n",
    "Note that here age is originally a continuous numerical feature. To use them in determining whether to use this feature as the splitting criteria we will often times group them into a small number of discrete categories. For instance, here we turned age into a binary class by indicating whether the person's age is 30 or younger or 31 or older.\n",
    "\n",
    "The decision tree algorithm will try **every possible split across all features**, and choose the split that **reduces the error rate the most.** In this case, it will split on gender."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another Kind of Splitting Criteria\n",
    "\n",
    "Another way to determine the best feature to split on is by choosing the one that maximizes the **Information Gain (IG)** at each split.\n",
    "\n",
    "$$IG(D_{p}, a) = I(D_{p}) - \\frac{N_{left}}{N_p} I(D_{left}) - \\frac{N_{right}}{N_p} I(D_{right})$$\n",
    "\n",
    "- $IG$: Information Gain.\n",
    "- $a$: feature to perform the split.\n",
    "- $N_p$: number of samples in the parent node.\n",
    "- $N_{left}$: number of samples in the left child node.\n",
    "- $N_{right}$: number of samples in the right child node.\n",
    "- $I$: Some impurity measure that we'll look at below (common ones include gini index, entropy).\n",
    "- $D_{p}$: training subset of the parent node.\n",
    "- $D_{left}$: training subset of the left child node.\n",
    "- $D_{right}$: training subset of the right child node.\n",
    "\n",
    "**[Gini Index](https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity)**\n",
    "\n",
    "This is a measure of how often a randomly chosen element from the set would be incorrectly labeled if it was randomly labeled according to the distribution of labels in the subset. Gini impurity can be computed by summing the probability $f_i$ of each item being chosen times the probability $1 − f_i$ of a mistake in categorizing that item. It reaches its minimum (zero) when all cases in the node fall into a single target category.\n",
    "\n",
    "To compute Gini impurity for a set of m items, suppose $i ∈ {1, 2, ..., m}$, and let $f_i$ be the fraction of items labeled with value $i$ in the set.\n",
    "\n",
    "$$ I = \\sum _{i=1}^{m}f_{i}(1-f_{i})=\\sum _{i=1}^{m}(f_{i}-{f_{i}}^{2})=\\sum _{i=1}^{m}f_{i}-\\sum _{i=1}^{m}{f_{i}}^{2}=1-\\sum _{i=1}^{m}{f_{i}}^{2}$$\n",
    "\n",
    "- The **maximum value** of the Gini index is 0.5, and occurs when the classes are perfectly balanced in a node.\n",
    "- The **minimum value** of the Gini index is 0, and occurs when there is only one class represented in a node.\n",
    "- A node with a lower Gini index is said to be more \"pure\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of Gini index\n",
    "\n",
    "Calculate the Gini index before making a split:\n",
    "\n",
    "$$1 - \\left(\\frac {iPhone} {Total}\\right)^2 - \\left(\\frac {Android} {Total}\\right)^2 = 1 - \\left(\\frac {10} {25}\\right)^2 - \\left(\\frac {15} {25}\\right)^2 = 0.48$$\n",
    "\n",
    "Evaluating the split on **gender** using Gini index:\n",
    "\n",
    "$$\\text{Males: } 1 - \\left(\\frac {2} {14}\\right)^2 - \\left(\\frac {12} {14}\\right)^2 = 0.24$$\n",
    "$$\\text{Females: } 1 - \\left(\\frac {8} {11}\\right)^2 - \\left(\\frac {3} {11}\\right)^2 = 0.40$$\n",
    "$$\\text{Weighted Average: } 0.24 \\left(\\frac {14} {25}\\right) + 0.40 \\left(\\frac {11} {25}\\right) = 0.31$$\n",
    "$$IG = 0.48 - 0.31 = 0.17$$\n",
    "\n",
    "Evaluating the split on **age** using Gini index:\n",
    "\n",
    "$$\\text{30 or younger: } 1 - \\left(\\frac {4} {12}\\right)^2 - \\left(\\frac {8} {12}\\right)^2 = 0.44$$\n",
    "$$\\text{31 or older: } 1 - \\left(\\frac {6} {13}\\right)^2 - \\left(\\frac {7} {13}\\right)^2 = 0.50$$\n",
    "$$\\text{Weighted Average: } 0.44 \\left(\\frac {12} {25}\\right) + 0.50 \\left(\\frac {13} {25}\\right) = 0.47$$\n",
    "$$IG = 0.48 - 0.47 = 0.01$$\n",
    "\n",
    "Again, the decision tree algorithm will try every possible split, and will choose the split that reduces the Gini index (and thus increases the \"node purity\") the most. Or in other words, maximizes the information gain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Impurity Functions $I$\n",
    "\n",
    "- Gini index is generally preferred because it will make splits that **increase node purity**, even if that split does not change the classification error rate.\n",
    "- Node purity is important because we're interested in the **class proportions** in each region, since that's how we calculate the **predicted probability** of each class.\n",
    "- scikit-learn's default splitting criteria for classification trees is Gini index.\n",
    "\n",
    "Note: There is another common splitting criteria called **entropy**. Gini index and entropy typically yield very similar results in terms of the model's prediction accuracy, thus it is often not worth spending time on tuning different impurity criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When To Stop Recursing\n",
    "\n",
    "There are some early stopping criteria that is commonly used to prevent the tree from overfitting.\n",
    "\n",
    "**Maximum depth** The length of the longest path from a root node to a leaf node will not exceed this value. This is the most commonly tuned hyperparameter for tree-based method.\n",
    "\n",
    "**Minimum node size:** This simply calculates whether the number of data points at a given node is less than or equal to the specified minimum node size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in the data\n",
    "titanic = pd.read_csv('titanic.csv')\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked_C</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>1</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name  Sex   Age  SibSp  Parch  \\\n",
       "0                            Braund, Mr. Owen Harris    1  22.0      1      0   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...    0  38.0      1      0   \n",
       "2                             Heikkinen, Miss. Laina    0  26.0      0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)    0  35.0      1      0   \n",
       "4                           Allen, Mr. William Henry    1  35.0      0      0   \n",
       "\n",
       "             Ticket     Fare Cabin  Embarked_C  Embarked_Q  Embarked_S  \n",
       "0         A/5 21171   7.2500   NaN         0.0         0.0         1.0  \n",
       "1          PC 17599  71.2833   C85         1.0         0.0         0.0  \n",
       "2  STON/O2. 3101282   7.9250   NaN         0.0         0.0         1.0  \n",
       "3            113803  53.1000  C123         0.0         0.0         1.0  \n",
       "4            373450   8.0500   NaN         0.0         0.0         1.0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encode female as 0 and male as 1\n",
    "titanic['Sex'] = titanic.Sex.map({ 'female': 0, 'male': 1 })\n",
    "\n",
    "# fill in the missing values for age with the median age\n",
    "titanic['Age'].fillna( titanic['Age'].median(), inplace = True )\n",
    "\n",
    "# one-hot encoded Embarked column\n",
    "embarked_dummies = pd.get_dummies( titanic['Embarked'], prefix = 'Embarked' )\n",
    "\n",
    "# concatenate the original DataFrame and the one-hot encoded DataFrame\n",
    "titanic.drop( 'Embarked', axis = 1, inplace = True )\n",
    "titanic = pd.concat([ titanic, embarked_dummies ], axis = 1 )\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define X and y\n",
    "features = ['Pclass', 'Sex', 'Age', 'Embarked_C', 'Embarked_Q', 'Embarked_S'] \n",
    "X = titanic[features]\n",
    "y = titanic['Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(712, 6)\n",
      "(179, 6)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size = 0.2, random_state = 1 )\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77094972067039103"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy score of one decision tree classifier\n",
    "tree = DecisionTreeClassifier( max_depth = 6 )\n",
    "tree.fit( X_train, y_train )\n",
    "y_pred = tree.predict(X_test)\n",
    "metrics.accuracy_score( y_true = y_test, y_pred = y_pred )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76536312849162014"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier( n_estimators = 30, max_depth = 6, max_features = 'sqrt' )\n",
    "rf.fit( X_train, y_train )\n",
    "y_pred = rf.predict(X_test)\n",
    "metrics.accuracy_score( y_true = y_test, y_pred = y_pred )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Implementation\n",
    "\n",
    "Link to download the data from [dropbox](https://www.dropbox.com/s/feo2z4wg8khcuct/lending-club-data.csv?dl=0). \n",
    "\n",
    "We will build a classification model to predict whether or not a loan provided by [LendingClub](https://www.lendingclub.com/) is likely to default. The dataset has feature columns that have to do with grade of the loan, annual income, home ownership status, etc. We can take a look at the distribution of the `home_ownership`, which describes whether the loanee is mortaging, renting, or owns a home."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MORTGAGE    59240\n",
      "RENT        53245\n",
      "OWN          9943\n",
      "OTHER         179\n",
      "Name: home_ownership, dtype: int64\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>member_id</th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>funded_amnt</th>\n",
       "      <th>funded_amnt_inv</th>\n",
       "      <th>term</th>\n",
       "      <th>int_rate</th>\n",
       "      <th>installment</th>\n",
       "      <th>grade</th>\n",
       "      <th>sub_grade</th>\n",
       "      <th>...</th>\n",
       "      <th>sub_grade_num</th>\n",
       "      <th>delinq_2yrs_zero</th>\n",
       "      <th>pub_rec_zero</th>\n",
       "      <th>collections_12_mths_zero</th>\n",
       "      <th>short_emp</th>\n",
       "      <th>payment_inc_ratio</th>\n",
       "      <th>final_d</th>\n",
       "      <th>last_delinq_none</th>\n",
       "      <th>last_record_none</th>\n",
       "      <th>last_major_derog_none</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1077501</td>\n",
       "      <td>1296599</td>\n",
       "      <td>5000</td>\n",
       "      <td>5000</td>\n",
       "      <td>4975</td>\n",
       "      <td>36 months</td>\n",
       "      <td>10.65</td>\n",
       "      <td>162.87</td>\n",
       "      <td>B</td>\n",
       "      <td>B2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.14350</td>\n",
       "      <td>20141201T000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1077430</td>\n",
       "      <td>1314167</td>\n",
       "      <td>2500</td>\n",
       "      <td>2500</td>\n",
       "      <td>2500</td>\n",
       "      <td>60 months</td>\n",
       "      <td>15.27</td>\n",
       "      <td>59.83</td>\n",
       "      <td>C</td>\n",
       "      <td>C4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.39320</td>\n",
       "      <td>20161201T000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1077175</td>\n",
       "      <td>1313524</td>\n",
       "      <td>2400</td>\n",
       "      <td>2400</td>\n",
       "      <td>2400</td>\n",
       "      <td>36 months</td>\n",
       "      <td>15.96</td>\n",
       "      <td>84.33</td>\n",
       "      <td>C</td>\n",
       "      <td>C5</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.25955</td>\n",
       "      <td>20141201T000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1076863</td>\n",
       "      <td>1277178</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "      <td>36 months</td>\n",
       "      <td>13.49</td>\n",
       "      <td>339.31</td>\n",
       "      <td>C</td>\n",
       "      <td>C1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.27585</td>\n",
       "      <td>20141201T000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1075269</td>\n",
       "      <td>1311441</td>\n",
       "      <td>5000</td>\n",
       "      <td>5000</td>\n",
       "      <td>5000</td>\n",
       "      <td>36 months</td>\n",
       "      <td>7.90</td>\n",
       "      <td>156.46</td>\n",
       "      <td>A</td>\n",
       "      <td>A4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.21533</td>\n",
       "      <td>20141201T000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 68 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  member_id  loan_amnt  funded_amnt  funded_amnt_inv        term  \\\n",
       "0  1077501    1296599       5000         5000             4975   36 months   \n",
       "1  1077430    1314167       2500         2500             2500   60 months   \n",
       "2  1077175    1313524       2400         2400             2400   36 months   \n",
       "3  1076863    1277178      10000        10000            10000   36 months   \n",
       "4  1075269    1311441       5000         5000             5000   36 months   \n",
       "\n",
       "   int_rate  installment grade sub_grade          ...          sub_grade_num  \\\n",
       "0     10.65       162.87     B        B2          ...                    0.4   \n",
       "1     15.27        59.83     C        C4          ...                    0.8   \n",
       "2     15.96        84.33     C        C5          ...                    1.0   \n",
       "3     13.49       339.31     C        C1          ...                    0.2   \n",
       "4      7.90       156.46     A        A4          ...                    0.8   \n",
       "\n",
       "  delinq_2yrs_zero pub_rec_zero  collections_12_mths_zero short_emp  \\\n",
       "0              1.0          1.0                       1.0         0   \n",
       "1              1.0          1.0                       1.0         1   \n",
       "2              1.0          1.0                       1.0         0   \n",
       "3              1.0          1.0                       1.0         0   \n",
       "4              1.0          1.0                       1.0         0   \n",
       "\n",
       "  payment_inc_ratio          final_d last_delinq_none last_record_none  \\\n",
       "0           8.14350  20141201T000000                1                1   \n",
       "1           2.39320  20161201T000000                1                1   \n",
       "2           8.25955  20141201T000000                1                1   \n",
       "3           8.27585  20141201T000000                0                1   \n",
       "4           5.21533  20141201T000000                1                1   \n",
       "\n",
       "  last_major_derog_none  \n",
       "0                     1  \n",
       "1                     1  \n",
       "2                     1  \n",
       "3                     1  \n",
       "4                     1  \n",
       "\n",
       "[5 rows x 68 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loans = pd.read_csv('lending-club-data.csv')\n",
    "\n",
    "# a small percentage of the loanees own a home\n",
    "print( loans['home_ownership'].value_counts() )\n",
    "print()\n",
    "loans.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target column (label column) of the dataset that we are interested in is called `bad_loans`. In this column **1** means a risky (bad) loan **0** means a safe loan. Here, we'll reassign the target to be:\n",
    "\n",
    "- **1** as a safe loan.\n",
    "- **0** as a risky (bad) loan. \n",
    "\n",
    "We put this in a new column called `safe_loans`. This step isn't necessary, it's more of a personal preference for the interpretation of the outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    99457\n",
       "0    23150\n",
       "Name: safe_loans, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loans['safe_loans'] = loans['bad_loans'].apply( lambda x: 1 if x == 0 else 0 )\n",
    "loans.drop( 'bad_loans', axis = 1, inplace = True )\n",
    "\n",
    "# it looks like most of these loans are safe loans (thankfully)\n",
    "# But this does make our problem of identifying risky loans challenging\n",
    "loans['safe_loans'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, we will just be using a subset of 4 categorical features: \n",
    "\n",
    "1. grade of the loan \n",
    "2. the length of the loan term\n",
    "3. the home ownership status: own, mortgage, rent\n",
    "4. number of years of employment.\n",
    "\n",
    "We'll also convert these categorical features to a binary representation using one-hot encoding. For instance, the **home_ownership** feature represents the home ownership status of the loanee, which is takes the value of either `own`, `mortgage` or `rent`. Now, if a data point has the feature: \n",
    "\n",
    "```\n",
    "{   'home_ownership': 'RENT'   }\n",
    "```\n",
    "\n",
    "we want to turn this into three features: \n",
    "\n",
    "```\n",
    "{ \n",
    "    'home_ownership = OWN'      : 0, \n",
    "    'home_ownership = MORTGAGE' : 0, \n",
    "    'home_ownership = RENT'     : 1\n",
    "}\n",
    "```\n",
    "Where the 1 denotes that the data point contains this feature, and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>safe_loans</th>\n",
       "      <th>grade_A</th>\n",
       "      <th>grade_B</th>\n",
       "      <th>grade_C</th>\n",
       "      <th>grade_D</th>\n",
       "      <th>grade_E</th>\n",
       "      <th>grade_F</th>\n",
       "      <th>grade_G</th>\n",
       "      <th>term_ 36 months</th>\n",
       "      <th>term_ 60 months</th>\n",
       "      <th>...</th>\n",
       "      <th>emp_length_2 years</th>\n",
       "      <th>emp_length_3 years</th>\n",
       "      <th>emp_length_4 years</th>\n",
       "      <th>emp_length_5 years</th>\n",
       "      <th>emp_length_6 years</th>\n",
       "      <th>emp_length_7 years</th>\n",
       "      <th>emp_length_8 years</th>\n",
       "      <th>emp_length_9 years</th>\n",
       "      <th>emp_length_&lt; 1 year</th>\n",
       "      <th>emp_length_n/a</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   safe_loans  grade_A  grade_B  grade_C  grade_D  grade_E  grade_F  grade_G  \\\n",
       "0           1      0.0      1.0      0.0      0.0      0.0      0.0      0.0   \n",
       "1           0      0.0      0.0      1.0      0.0      0.0      0.0      0.0   \n",
       "2           1      0.0      0.0      1.0      0.0      0.0      0.0      0.0   \n",
       "3           1      0.0      0.0      1.0      0.0      0.0      0.0      0.0   \n",
       "4           1      1.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "   term_ 36 months  term_ 60 months       ...        emp_length_2 years  \\\n",
       "0              1.0              0.0       ...                       0.0   \n",
       "1              0.0              1.0       ...                       0.0   \n",
       "2              1.0              0.0       ...                       0.0   \n",
       "3              1.0              0.0       ...                       0.0   \n",
       "4              1.0              0.0       ...                       0.0   \n",
       "\n",
       "   emp_length_3 years  emp_length_4 years  emp_length_5 years  \\\n",
       "0                 0.0                 0.0                 0.0   \n",
       "1                 0.0                 0.0                 0.0   \n",
       "2                 0.0                 0.0                 0.0   \n",
       "3                 0.0                 0.0                 0.0   \n",
       "4                 1.0                 0.0                 0.0   \n",
       "\n",
       "   emp_length_6 years  emp_length_7 years  emp_length_8 years  \\\n",
       "0                 0.0                 0.0                 0.0   \n",
       "1                 0.0                 0.0                 0.0   \n",
       "2                 0.0                 0.0                 0.0   \n",
       "3                 0.0                 0.0                 0.0   \n",
       "4                 0.0                 0.0                 0.0   \n",
       "\n",
       "   emp_length_9 years  emp_length_< 1 year  emp_length_n/a  \n",
       "0                 0.0                  0.0             0.0  \n",
       "1                 0.0                  1.0             0.0  \n",
       "2                 0.0                  0.0             0.0  \n",
       "3                 0.0                  0.0             0.0  \n",
       "4                 0.0                  0.0             0.0  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract the subset of feature columns and the target column\n",
    "features = [ 'grade', 'term', 'home_ownership', 'emp_length' ]\n",
    "target = 'safe_loans'\n",
    "loans  = loans[ features + [target] ]\n",
    "\n",
    "# one-hot encoding\n",
    "loans = pd.get_dummies(loans)\n",
    "loans.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(98085, 25)\n",
      "(24522, 25)\n"
     ]
    }
   ],
   "source": [
    "# obtain the features after one-hot encoded,\n",
    "# the first column is the target\n",
    "features = loans.columns[1:]\n",
    "X = loans[features].values\n",
    "y = loans[target].values\n",
    "\n",
    "# train / test split\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size = 0.2, random_state = 1 )\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80976266209933934"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy score of one decision tree classifier\n",
    "tree = DecisionTreeClassifier( max_depth = 6 )\n",
    "tree.fit( X_train, y_train )\n",
    "y_pred = tree.predict(X_test)\n",
    "metrics.accuracy_score( y_true = y_test, y_pred = y_pred )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80980344180735664"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier( n_estimators = 50, max_depth = 6, max_features = 'sqrt' )\n",
    "rf.fit( X_train, y_train )\n",
    "y_pred = rf.predict(X_test)\n",
    "metrics.accuracy_score( y_true = y_test, y_pred = y_pred )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def best_splitting_feature( X, y ):\n",
    "    \"\"\"\n",
    "    Loop through the possible feature to consider splitting on each on them\n",
    "    and return the best feature we found. Where best is determined by the \n",
    "    maximum information gain that uses gini index as the impurity score\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X, y : np-array\n",
    "        data and its target value\n",
    "        \n",
    "    features: list\n",
    "        candidates (columns) to be considered for splits\n",
    "        \n",
    "    target: string\n",
    "        name of the target/label column\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    best_feature: int\n",
    "        column index of the best feature we've found\n",
    "    \"\"\"\n",
    "    \n",
    "    # keep track of the best score for the splitting criteria and its corresponding feature\n",
    "    # we'll simply intialize the best score to be some small number\n",
    "    best_feature = None\n",
    "    best_score = -np.inf\n",
    "    \n",
    "    # compute the impurity score, here in gini index for the current data points\n",
    "    original_score = compute_node_score(y)\n",
    "    data_num, feature_num = X.shape\n",
    "\n",
    "    for f in range(feature_num):\n",
    "              \n",
    "        # Since we've one hot encoded the data points,\n",
    "        # the left split will have all data points where the feature value is 0 and\n",
    "        # the right split will have all data points where the feature value is 1\n",
    "        split = X[ :, f ]\n",
    "        y_left  = y[ split == 0 ]\n",
    "        y_right = y[ split == 1 ] \n",
    "            \n",
    "        # compute the impurity score for the left and right split\n",
    "        left_score  = compute_node_score( y = y_left  )\n",
    "        right_score = compute_node_score( y = y_right )\n",
    "        \n",
    "        # compute the information gain     \n",
    "        left_data_num  = y_left.shape[0]\n",
    "        right_data_num = y_right.shape[0]        \n",
    "        split_score = ( ( right_data_num / data_num ) * right_score + \n",
    "                        ( left_data_num / data_num ) * left_score )\n",
    "        score = original_score - split_score\n",
    "\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_feature = f\n",
    "    \n",
    "    return best_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_fraction(y):\n",
    "    data_num = y.shape[0]\n",
    "    labels_num = np.bincount(y)\n",
    "    fraction = labels_num / data_num\n",
    "    return fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_node_score(y):\n",
    "    # print(y)\n",
    "    \"\"\"compute the impurity score, the gini index of the current node\"\"\"\n",
    "    fraction = compute_fraction(y)\n",
    "    gini_index = 1 - np.sum( fraction ** 2 )\n",
    "    return gini_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_leaf(y):\n",
    "    \n",
    "    # add the predicted probability of each label\n",
    "    # to the dictionary, which is simply the fraction\n",
    "    fraction = compute_fraction(y)\n",
    "    leaf = { \n",
    "        'is_leaf'   : True,  \n",
    "        'prediction': fraction\n",
    "    }\n",
    "    return leaf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fit( X, y ):\n",
    "    feature_num = X.shape[1]\n",
    "    features = list( range(feature_num) )    \n",
    "    node = create_decision_tree( X = X, y = y, features = features )   \n",
    "    return node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_decision_tree( X, y, features, current_depth = 0 ):\n",
    "    \n",
    "    # make a copy of the features\n",
    "    remaining_features = features[:] \n",
    "       \n",
    "    # define stopping condition, \n",
    "    # if it is reached, make the current node a leaf node\n",
    "    \n",
    "    # if the number of data points is less than or equal to the minimum size\n",
    "    if X.shape[0] <= min_node_size:\n",
    "        return create_leaf(y)\n",
    "    \n",
    "    # if there are no remaining features to consider splitting on for one branch\n",
    "    if not remaining_features:\n",
    "        return create_leaf(y)    \n",
    "    \n",
    "    # limit the tree depth\n",
    "    if current_depth >= max_depth:\n",
    "        return create_leaf(y)\n",
    "\n",
    "    # find the best splitting feature and split on it\n",
    "    splitting_feature = best_splitting_feature( X, y )\n",
    "    split = X[ :, splitting_feature ]\n",
    "    X_left, X_right = X[ split == 0 ], X[ split == 1 ]\n",
    "    y_left, y_right = y[ split == 0 ], y[ split == 1 ]\n",
    "        \n",
    "    # recursively build the left and right sub-trees\n",
    "    remaining_features = list( set(remaining_features) - set([splitting_feature]) )\n",
    "    \n",
    "    left_tree = create_decision_tree( \n",
    "        X = X_left,\n",
    "        y = y_left,\n",
    "        features = remaining_features,     \n",
    "        current_depth = current_depth + 1, \n",
    "    )        \n",
    "    right_tree = create_decision_tree( \n",
    "        X = X_right,\n",
    "        y = y_right,\n",
    "        features = remaining_features,        \n",
    "        current_depth = current_depth + 1\n",
    "    )\n",
    "    \n",
    "    node = { \n",
    "        'is_leaf'          : False, \n",
    "        'left'             : left_tree,\n",
    "        'right'            : right_tree,\n",
    "        'splitting_feature': splitting_feature\n",
    "    }\n",
    "    return node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify( x, tree ):   \n",
    "\n",
    "    if tree['is_leaf']:\n",
    "        return tree['prediction'] \n",
    "    else:\n",
    "        # split on feature, splitted feature value of 0 is grown on the left-subtree\n",
    "        split_feature_value = x[ tree['splitting_feature'] ]\n",
    "        if split_feature_value == 0:\n",
    "            return classify( x, tree['left'] )\n",
    "        else:\n",
    "            return classify( x, tree['right'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "min_node_size = 10\n",
    "max_depth = 6\n",
    "\n",
    "tree_model = fit( X_train, y_train )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction = np.apply_along_axis( func1d = classify, axis = 1, arr = X_test, tree = tree_model )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.27610759,  0.72389241],\n",
       "       [ 0.07545563,  0.92454437],\n",
       "       [ 0.15343811,  0.84656189],\n",
       "       ..., \n",
       "       [ 0.06092199,  0.93907801],\n",
       "       [ 0.27610759,  0.72389241],\n",
       "       [ 0.12392451,  0.87607549]])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Advantages of decision trees:**\n",
    "\n",
    "- Can be displayed graphically, thus making it highly interpretable.\n",
    "- Features don't need scaling or normalization.\n",
    "- Tends to ignore irrelevant features.\n",
    "- Non-parametric (will outperform linear models if relationship between features and response is highly non-linear)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Methods\n",
    "\n",
    "Ensembling is a very popular method for improving the predictive performance of machine learning models. Let's pretend that instead of building a single model to solve a binary classification problem, you created **five independent models**, and each model was correct about 70% of the time. If you combined these models into an \"ensemble\" and used their majority vote as a prediction, how often would the ensemble be correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1]\n",
      "[1 1 1 1 1 1 1 0 1 0 0 0 1 1 1 0 1 0 0 0]\n",
      "[1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1]\n",
      "[1 1 0 0 0 0 1 1 0 1 1 1 1 1 1 0 1 1 1 0]\n",
      "[0 0 1 0 0 0 1 0 1 0 0 0 1 1 1 1 1 1 1 1]\n",
      "0.713\n",
      "0.665\n",
      "0.717\n",
      "0.712\n",
      "0.687\n"
     ]
    }
   ],
   "source": [
    "# generate 1000 random numbers (between 0 and 1) for each model, representing 1000 observations\n",
    "np.random.seed(1234)\n",
    "mod1 = np.random.rand(1000)\n",
    "mod2 = np.random.rand(1000)\n",
    "mod3 = np.random.rand(1000)\n",
    "mod4 = np.random.rand(1000)\n",
    "mod5 = np.random.rand(1000)\n",
    "\n",
    "# each model independently predicts 1 (the \"correct response\") if random number was at least 0.3\n",
    "preds1 = np.where(mod1 > 0.3, 1, 0)\n",
    "preds2 = np.where(mod2 > 0.3, 1, 0)\n",
    "preds3 = np.where(mod3 > 0.3, 1, 0)\n",
    "preds4 = np.where(mod4 > 0.3, 1, 0)\n",
    "preds5 = np.where(mod5 > 0.3, 1, 0)\n",
    "\n",
    "# print the first 20 predictions from each model\n",
    "print(preds1[:20])\n",
    "print(preds2[:20])\n",
    "print(preds3[:20])\n",
    "print(preds4[:20])\n",
    "print(preds5[:20])\n",
    "\n",
    "# how accurate was each individual model?\n",
    "print(preds1.mean())\n",
    "print(preds2.mean())\n",
    "print(preds3.mean())\n",
    "print(preds4.mean())\n",
    "print(preds5.mean()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1]\n",
      "0.841\n"
     ]
    }
   ],
   "source": [
    "# average the predictions, and then round to 0 or 1\n",
    "# you can also do a weighted average, as long as the weight adds up to 1 \n",
    "ensemble_preds = np.round((preds1 + preds2 + preds3 + preds4 + preds5) / 5).astype(int)\n",
    "\n",
    "# print the ensemble's first 20 predictions\n",
    "print(ensemble_preds[:20])\n",
    "\n",
    "# how accurate was the ensemble?\n",
    "print(ensemble_preds.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ensemble learning (or \"ensembling\")** is the process of combining several predictive models in order to produce a combined model that is more accurate than any individual model.\n",
    "\n",
    "- **Regression:** take the average of the predictions\n",
    "- **Classification:** take a vote and use the most common prediction, or take the average of the predicted probabilities\n",
    "\n",
    "The big idea: If you have a collection of individually imperfect, but different models (by different, it might mean that it uses different hyperparameters, features or class of algorithms), the \"one-off\" mistakes made by each model are probably not going to be made by the rest of the models, and thus the mistakes will be discarded when averaging the models.\n",
    "\n",
    "There are two basic methods for ensembling:\n",
    "\n",
    "- Manually ensemble your individual models (e.g. the weighted average method above).\n",
    "- Use a model that ensembles for you, which we'll see below.\n",
    "\n",
    "\n",
    "## Randomforest\n",
    "\n",
    "The primary weakness of **decision trees** is that they usually suffer from high variance and don't tend to have the best predictive accuracy. This means that if we split the training data into two parts at random, and fit a decision tree to both halves, the results that we get could be quite different. Thus in a **randomforest**, we will grow multiple INDEPENDENT decision trees as opposed to a single tree. Then, to classify a new object based on attributes, the forest chooses the classification having the most votes or the weighted probability (over all the trees in the forest) and in case of regression, it takes the average of outputs by different trees.\n",
    "\n",
    "In randomforest, each tree is trained as follows:\n",
    "\n",
    "- Assume number of observations in the training set is $N$. Then, sample of these $N$ cases is taken at random but with replacement (i.e. bagging or bootstrap aggregation). This sample will be the training set for growing the tree. Notice here, each tree in randomforest is built on a different bootstrap data set, making it independent of all the other trees.\n",
    "- If there are $M$ total input variables (features), a number of $ m < M$ features is chosen randomly to become our split candidates from the full set of $M$ total features (without replacement this time). The best split on these $m$ is then used to split the node. The value of $m$ is held constant while growing each tree. In the regression context, it is suggested to set $m$ to be one-third of $M$. As for classification, square root of $M$ is usually the default. The point of doing this is that, suppose there is one very strong feature in the data set. When training the trees, most of the trees will use that feature as the top split, resulting in an ensemble of similar trees that are highly correlated. By randomly leaving out candidate features from each split, Random Forests \"decorrelates\" the trees, such that the averaging process can reduce the variance of the resulting model.\n",
    "- Each tree is grown to the largest extent possible (you can specify the max_depth parameter to control how large) and there is no pruning.\n",
    "\n",
    "In the end, we'll predict new data by aggregating the predictions of the number trees (e.g., majority votes for classification, average for regression)\n",
    "\n",
    "If this makes absolutely no sense to you, try this more intuitive explanation from [Quora](https://www.quora.com/How-does-randomization-in-a-random-forest-work?redirected_qid=212859).\n",
    "\n",
    "**Toy Implementation:** Built on top of scikit learn's decision tree.\n",
    "\n",
    "The example [digit dataset](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html) is made up of 1797 8x8 images. Each image, is of a hand-written digit. In order to utilize an 8x8 figure like this, we’d have to first transform it into a feature vector with length 64. There're a total of 1797 examples and 10 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1437, 64)\n",
      "(360, 64)\n"
     ]
    }
   ],
   "source": [
    "# load the data and split it into training and testing\n",
    "digits = load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size = 0.2, random_state = 1 )\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80906940706304542"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy score of one decision tree classifier\n",
    "tree = DecisionTreeClassifier()\n",
    "tree.fit( X_train, y_train )\n",
    "y_pred = tree.predict(X_test)\n",
    "metrics.accuracy_score( y_true = y_test, y_pred = y_pred )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Side note, the best thing about decision trees is that can be displayed graphically, and are easily interpreted even by a non-expert. Thus the following section provides a code for visualizing the tree. Note that you'll need to have [graphviz](http://www.graphviz.org/) installed. Or this [link](http://macappstore.org/graphviz-2/) if you're a mac user.\n",
    "\n",
    "```python\n",
    "\n",
    "from sklearn.tree import export_graphviz\n",
    "import graphviz\n",
    "\n",
    "# export it as .dot file, other common parameters include\n",
    "# `feature_names` \n",
    "# `fill` (for filling the node with different output type)\n",
    "# `class_names`\n",
    "# `rounded` (boolean to round the score on each node)\n",
    "export_graphviz( tree, out_file = \"tree.dot\" )\n",
    "\n",
    "# read it in and visualize it\n",
    "with open(\"tree.dot\") as f:\n",
    "    dot_graph = f.read()\n",
    "graphviz.Source(dot_graph)\n",
    "\n",
    "# or if you wish to convert the .dot file into other formats\n",
    "import os\n",
    "os.system(\"dot -Tpng tree.dot -o tree.jpeg\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RandomForest(object):\n",
    "    \"\"\"\n",
    "    vanilla classification random forest\n",
    "    \n",
    "    the two core features for random forest\n",
    "    is the n_estimators, the number of trees that you're going built\n",
    "    and the max_features, the number of features that you allow\n",
    "    when deciding which feature to split on, you can still specify\n",
    "    all the other parameters for a decision tree like math_depth\n",
    "    or min_sample_split, it is just not used here as that is more\n",
    "    related to a single decision tree\n",
    "    \"\"\"   \n",
    "    def __init__( self, n_estimators, max_features ):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_features = max_features\n",
    "        \n",
    "    \n",
    "    def fit( self, X, y ):       \n",
    "        # draw the bootstrap samples\n",
    "        n_samples = y.shape[0]\n",
    "           \n",
    "        # for each number of base-tree models to build\n",
    "        # 1. draw bootstrap samples from the original data\n",
    "        # 2. train a tree model on that bootstrap sample,\n",
    "        #    during training, randomly select a number of features to split on each node\n",
    "        self.forest = []\n",
    "        for _ in range(self.n_estimators):\n",
    "            bootstrap = np.random.choice( n_samples, size = n_samples, replace = True )\n",
    "            X_boostrap = X[bootstrap]\n",
    "            y_boostrap = y[bootstrap]\n",
    "            \n",
    "            # using scikit learn's decision tree as the base tree\n",
    "            tree = DecisionTreeClassifier( max_features = self.max_features )\n",
    "            tree.fit( X_boostrap, y_boostrap )\n",
    "            self.forest.append(tree)\n",
    "            \n",
    "    \n",
    "    def predict( self, X ):        \n",
    "        # obtain the prediction for each base-tree model\n",
    "        # then use majority vote to decide the predicted class\n",
    "        # for each observation\n",
    "        n_samples = X.shape[0]\n",
    "        tree_pred = np.zeros( ( n_samples, self.n_estimators ), dtype = np.int )       \n",
    "        for j in range(self.n_estimators):\n",
    "            tree = self.forest[j]\n",
    "            tree_pred[ :, j ] = tree.predict(X)\n",
    "\n",
    "        y_pred = np.zeros(n_samples)\n",
    "        for i in range(n_samples):\n",
    "            y_pred[i] = np.argmax( np.bincount( tree_pred[ i, : ] ) )\n",
    "        \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "indices are out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-e52d0961f412>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# accuracy score of random forest classifier using 30 decision trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mrf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForest\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mn_estimators\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'sqrt'\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-76b59455a7e0>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mbootstrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mX_boostrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbootstrap\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0my_boostrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbootstrap\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ethen/anaconda/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1984\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1985\u001b[0m             \u001b[0;31m# either boolean or fancy integer index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1986\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1987\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1988\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ethen/anaconda/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_array\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2029\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2030\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2031\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2032\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2033\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ethen/anaconda/lib/python3.5/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, indices, axis, convert, is_copy)\u001b[0m\n\u001b[1;32m   1626\u001b[0m         new_data = self._data.take(indices,\n\u001b[1;32m   1627\u001b[0m                                    \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_block_manager_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1628\u001b[0;31m                                    convert=True, verify=True)\n\u001b[0m\u001b[1;32m   1629\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ethen/anaconda/lib/python3.5/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, indexer, axis, verify, convert)\u001b[0m\n\u001b[1;32m   3635\u001b[0m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3636\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3637\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_convert_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mverify\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ethen/anaconda/lib/python3.5/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36mmaybe_convert_indices\u001b[0;34m(indices, n)\u001b[0m\n\u001b[1;32m   1808\u001b[0m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1810\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"indices are out-of-bounds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1811\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: indices are out-of-bounds"
     ]
    }
   ],
   "source": [
    "# accuracy score of random forest classifier using 30 decision trees\n",
    "rf = RandomForest( n_estimators = 30, max_features = 'sqrt' )\n",
    "rf.fit( X_train, y_train )\n",
    "y_pred = rf.predict(X_test)\n",
    "metrics.accuracy_score( y_true = y_test, y_pred = y_pred )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting (Regression)\n",
    "\n",
    "**Gradient boosting machines** is a method stemmed from boosting. The basic principles of the algorithm are as follows: given a loss function (e.g., squared error for regression) and a weak learner (e.g., regression trees), the algorithm seeks to find an additive model that minimizes the loss function. The algorithm is typically initialized with the best guess of the response (e.g., the mean of the response in regression). The gradient (e.g., residual) is calculated, and a model is then fit to the residuals to minimize the loss function. The current model is added to the previous model, and the procedure continues for a user-specified number of iterations.\n",
    "\n",
    "If squared error is used as the loss function, then a simple psuedocode for the algorithm is listed below.\n",
    "\n",
    "- Compute the average response, $\\bar{y}$, and use this as the initial predicted value for each sample\n",
    "- for $b = 1, 2, ..., B$: ($B$ is the number of boosted trees, or simply think of it as iterations)\n",
    "    - Compute the residual, the difference between the observed value and the current predicted value, for each sample\n",
    "    Fit a regression tree of depth, D, using the residuals as the response\n",
    "    Predict each sample using the regression tree fit in the previous step\n",
    "    Update the predicted value of each sample by adding the previous iteration's predicted value to the predicted value \n",
    "    generated in the previous step\n",
    "\n",
    "Clearly, this version of boosting has similarities to random forests: the final prediction is based on an ensemble of models, and trees are used as the base learner. However, the way the ensembles are constructed differs substantially between each model. In random forests, all trees are created independently, which means that each tree contributes equally to the final model. The trees in boosting, however, are dependent on past trees and contribute unequally to the final model. Despite these differences, both random forests and boosting offer competitive predictive performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404, 13)\n",
      "(102, 13)\n"
     ]
    }
   ],
   "source": [
    "# Load boston housing dataset as an example\n",
    "boston = load_boston()\n",
    "X = boston[\"data\"]\n",
    "y = boston[\"target\"]\n",
    "names = boston[\"feature_names\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size = 0.2, random_state = 1 )\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.824803921568627"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decision_tree = DecisionTreeRegressor()\n",
    "decision_tree.fit( X_train, y_train )   \n",
    "y_pred = decision_tree.predict(X_test)\n",
    "metrics.mean_squared_error( y_true = y_test, y_pred = y_pred )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# number of trees to train,\n",
    "# or in xgboost, the num_boost_round\n",
    "n_estimators = 30\n",
    "\n",
    "# learning rate\n",
    "eta = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.12239174738148"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "trees = []\n",
    "\n",
    "residuals = y_train.copy()\n",
    "for i in range(n_estimators):\n",
    "    tree = DecisionTreeRegressor()\n",
    "    tree.fit( X_train, residuals )   \n",
    "    predictions = tree.predict(X_train)\n",
    "    \n",
    "    trees.append(tree)\n",
    "    \n",
    "    residuals -= eta * predictions\n",
    "\n",
    "y_pred = np.zeros(X_test.shape[0])\n",
    "for tree in trees:\n",
    "    y_pred += eta * tree.predict(X_test)\n",
    "\n",
    "metrics.mean_squared_error( y_true = y_test, y_pred = y_pred )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance\n",
    "\n",
    "Typically randomforest results in improved accuracy over prediction using a single tree. Unfortunately, it can be difficult to interpret the resulting model since we're basically training a whole bunch of decision trees and averaging their \"decisions\". We can still, however, obtain an overall summary of the importance of each predictor using **mean decrease impurity** or **mean decrease accuracy**.\n",
    "\n",
    "We'll start with **mean decrease impurity**. Recall that training a decision tree, every node in the tree is a condition on a single feature, designed to split the dataset into two so that similar response values end up in the same set. During this process, the node is chosen based on the (local) optimal decrease in impurity, which is typically either Gini impurity or information gain/entropy for classification trees and RSS (residual sum of squares )for regression trees. Thus when training a tree, it can be recorded how much each feature decreases the impurity.\n",
    "\n",
    "Now that we've have this notion, we can extend the idea to randomforest. Since it is basically consists of a number of decision trees, the impurity decrease from each feature can simply be averaged over all the trees and the features are then ranked according to this measure (A large value indicates an important predictor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404, 13)\n",
      "(102, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>imp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LSTAT</td>\n",
       "      <td>0.483755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>RM</td>\n",
       "      <td>0.304860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>DIS</td>\n",
       "      <td>0.058533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>CRIM</td>\n",
       "      <td>0.046649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NOX</td>\n",
       "      <td>0.030843</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature       imp\n",
       "12   LSTAT  0.483755\n",
       "11      RM  0.304860\n",
       "10     DIS  0.058533\n",
       "9     CRIM  0.046649\n",
       "8      NOX  0.030843"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestRegressor( n_estimators = 30 )\n",
    "rf.fit( X_train, y_train )\n",
    "imp = rf.feature_importances_\n",
    "\n",
    "indices = np.argsort(imp)\n",
    "imp_df = pd.DataFrame({ 'feature': names[indices], 'imp': imp[indices] })\n",
    "imp_df.sort_values( 'imp', ascending = False, inplace = True )\n",
    "imp_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few gotchas to keep in mind when using the impurity based ranking. \n",
    "\n",
    "- Firstly, feature selection based on impurity reduction is biased towards preferring variables with more categories. \n",
    "- Secondly, when the dataset has two (or more) correlated features, then from the point of view of the model, any of these correlated features can be used as the predictor, with no concrete preference of one over the others. But once one of them is used, the importance of others is significantly reduced since effectively the impurity they can remove is already removed by the first feature. As a consequence, they will have a lower reported importance. This is not an issue when we want to use feature selection to reduce overfitting, since it makes sense to remove features that are mostly duplicated by other features. But when interpreting the data, it can lead to the incorrect conclusion that one of the variables is a strong predictor while the others in the same group are unimportant, while actually they are very close in terms of their relationship with the response variable. We'll show this notion with a toy example below.\n",
    "\n",
    "In the following example, we have three correlated variables X0,X1,X2, with the output variable simply being the sum of the three features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores for X0, X1, X2: [ 0.27227966  0.54196882  0.18575152]\n"
     ]
    }
   ],
   "source": [
    "size = 10000\n",
    "np.random.seed(seed = 10)\n",
    "X_seed = np.random.normal(0, 1, size)\n",
    "X0 = X_seed + np.random.normal(0, .1, size)\n",
    "X1 = X_seed + np.random.normal(0, .1, size)\n",
    "X2 = X_seed + np.random.normal(0, .1, size)\n",
    "X = np.array([X0, X1, X2]).T\n",
    "Y = X0 + X1 + X2\n",
    "  \n",
    "# 20 trees, at each split, only two of the three features are considered\n",
    "rf_reg = RandomForestRegressor(n_estimators = 20, max_features = 2 )\n",
    "rf_reg.fit( X, Y )\n",
    "print( \"Scores for X0, X1, X2:\", rf_reg.feature_importances_ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we compute the feature importances, we see that X1 is computed to have over 10x higher importance than X2, while their \"true\" importance should be very similar. One thing to point out though is that the difficulty of interpreting the importance/ranking of correlated variables is not random forest specific, but applies to most model based feature selection methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mean decrease accuracy**\n",
    "\n",
    "Another feature selection method is to directly measure the impact of each feature on accuracy of the model. The general idea is to permute the values of each feature and measure how much the permutation decreases the accuracy of the model. Clearly, for unimportant variables, the permutation should have little to no effect on model accuracy, while permuting important variables should significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def feature_importance( rf, X, y ):\n",
    "    \n",
    "    y_pred = rf.predict(X)\n",
    "    original_score = metrics.r2_score( y_true = y, y_pred = y_pred )\n",
    "    \n",
    "    imp = []\n",
    "    for j in range(X.shape[1]):\n",
    "        \n",
    "        # copy the original data and shuffle one of the feauture's row order\n",
    "        X_copy = X.copy()\n",
    "        np.random.shuffle( X_copy[ :, j ] )\n",
    "        \n",
    "        y_pred = rf.predict(X_copy)\n",
    "        shuffled_score = metrics.r2_score( y_true = y, y_pred = y_pred )\n",
    "        diff_score = ( original_score - shuffled_score ) / original_score       \n",
    "        imp.append(diff_score)\n",
    "        \n",
    "    return imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.036421945019896089,\n",
       " -0.0013419756313426251,\n",
       " -0.00065772953953957948,\n",
       " 0.00030352150544105734,\n",
       " 0.030569578679355832,\n",
       " 0.54667509315923135,\n",
       " 0.0059797550370029604,\n",
       " 0.059004156657436767,\n",
       " -0.0024793839798306124,\n",
       " 0.0056676481991816364,\n",
       " 0.055628170042757344,\n",
       " 0.011605390220211146,\n",
       " 0.94217739292137237]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp = feature_importance( rf, X = X_test, y = y_test )\n",
    "imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features sorted by their score:\n",
      "[(0.75180000000000002, 'LSTAT'), (0.52600000000000002, 'RM'), (0.088200000000000001, 'DIS'), (0.042599999999999999, 'NOX'), (0.042000000000000003, 'CRIM'), (0.019099999999999999, 'PTRATIO'), (0.0167, 'TAX'), (0.010999999999999999, 'AGE'), (0.0055999999999999999, 'B'), (0.0041999999999999997, 'INDUS'), (0.0030000000000000001, 'RAD'), (0.00029999999999999997, 'CHAS'), (0.0, 'ZN')]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import ShuffleSplit\n",
    "from sklearn.metrics import r2_score\n",
    "from collections import defaultdict\n",
    "from sklearn.datasets import load_boston\n",
    "# Load boston housing dataset as an example\n",
    "boston = load_boston()\n",
    "X = boston[\"data\"]\n",
    "Y = boston[\"target\"]\n",
    "names = boston[\"feature_names\"]\n",
    " \n",
    "rf = RandomForestRegressor()\n",
    "scores = defaultdict(list)\n",
    " \n",
    "#crossvalidate the scores on a number of different random splits of the data\n",
    "for train_idx, test_idx in ShuffleSplit(len(X), 100, .3):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    Y_train, Y_test = Y[train_idx], Y[test_idx]\n",
    "    r = rf.fit(X_train, Y_train)\n",
    "    acc = r2_score(Y_test, rf.predict(X_test))\n",
    "    for i in range(X.shape[1]):\n",
    "        X_t = X_test.copy()\n",
    "        np.random.shuffle(X_t[:, i])\n",
    "        shuff_acc = r2_score(Y_test, rf.predict(X_t))\n",
    "        scores[names[i]].append((acc-shuff_acc)/acc)\n",
    "print( \"Features sorted by their score:\" )\n",
    "print(sorted([(round(np.mean(score), 4), feat) for\n",
    "              feat, score in scores.items()], reverse=True) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example LSTAT and RM are two features that strongly impact model performance: permuting them decreases model performance by ~73% and ~57% respectively. Keep in mind though that these measurements are made only after the model has been trained (and is depending) on all of these features. This doesn’t mean that if we train the model without one these feature, the model performance will drop by that amount, since other, correlated features can be used instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "- Decision Tree\n",
    "    - [Coursersa's Course: Washington Classification](https://www.coursera.org/learn/ml-classification)\n",
    "    - [A Visual Introduction to Machine Learning](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/)\n",
    "    - [Cheatsheet for Decision Tree Classification](http://nbviewer.jupyter.org/github/rasbt/pattern_classification/blob/master/machine_learning/decision_trees/decision-tree-cheatsheet.ipynb)\n",
    "    - [General Assembly's Notebook On Decision tree](http://nbviewer.jupyter.org/github/justmarkham/DAT8/blob/master/notebooks/17_decision_trees.ipynb)\n",
    "- Randomforest\n",
    "    - [Analyticsvidhya: randomforest](http://www.analyticsvidhya.com/blog/2015/09/random-forest-algorithm-multiple-challenges/)\n",
    "    - [Quora: How does randomization in a random forest work?](https://www.quora.com/How-does-randomization-in-a-random-forest-work?redirected_qid=212859)\n",
    "    - [General Assembly's Notebook On Ensembling](http://nbviewer.jupyter.org/github/justmarkham/DAT8/blob/master/notebooks/18_ensembling.ipynb)\n",
    "    - [Blog: Selecting good features with random forests](http://blog.datadive.net/selecting-good-features-part-iii-random-forests/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.quora.com/What-is-an-intuitive-explanation-of-Gradient-Boosting\n",
    "\n",
    "http://stats.stackexchange.com/questions/135378/how-are-individual-trees-added-together-in-boosted-regression-tree\n",
    "\n",
    "https://www.youtube.com/watch?v=sRktKszFmSk\n",
    "\n",
    "http://nbviewer.jupyter.org/github/leig/Applied-Predictive-Modeling-with-Python/blob/master/notebooks/Chapter%208.ipynb\n",
    "\n",
    "http://localhost:8888/notebooks/machine-learning/trees/Chapter%208.ipynb\n",
    "\n",
    "http://www.ccs.neu.edu/home/vip/teach/MLcourse/4_boosting/slides/gradient_boosting.pdf\n",
    "\n",
    "### variable importance\n",
    "\n",
    "http://stats.stackexchange.com/questions/162162/relative-variable-importance-for-boosting\n",
    "    \n",
    "http://papers.nips.cc/paper/4928-understanding-variable-importances-in-forests-of-randomized-trees.pdf\n",
    "\n",
    "\n",
    "\n",
    "http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#varimp\n",
    "\n",
    "\n",
    "\n",
    "### trees\n",
    "\n",
    "http://localhost:8888/notebooks/washington-machine-learning-specialization/course-3/module-6-decision-tree-practical-assignment.ipynb\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
